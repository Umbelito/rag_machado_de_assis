# Teste Comparativo: LLM 3B vs LLM 1B + RAG

Este projeto implementa um teste comparativo entre duas abordagens de LLMs para responder perguntas sobre obras de Machado de Assis:

## ğŸ¯ Objetivo

Comparar o desempenho de:
- **LLM de 3B parÃ¢metros** (sem RAG) - geraÃ§Ã£o pura baseada no conhecimento prÃ©-treinado
- **LLM de 1B parÃ¢metros** (com RAG) - geraÃ§Ã£o baseada em contexto recuperado de um banco vetorial

## ğŸ—ï¸ Arquitetura

### LLM de 3B (sem RAG)
- Modelo: `llama2:3b`
- Abordagem: GeraÃ§Ã£o pura sem contexto adicional
- Prompt: InstruÃ§Ã£o simples para responder como especialista em literatura

### LLM de 1B (com RAG)
- Modelo: `tinyllama`
- Abordagem: Retrieval-Augmented Generation (RAG)
- Banco vetorial: FAISS com embeddings de trechos das obras
- Prompt: Contexto especÃ­fico + pergunta

## ğŸ“ Estrutura do Projeto

```
â”œâ”€â”€ machado_rag.py          # Script principal do teste comparativo
â”œâ”€â”€ utils.py               # FunÃ§Ãµes auxiliares
â”œâ”€â”€ setup_models.py        # Script para configurar modelos
â”œâ”€â”€ analisar_resultados.py # Script para analisar resultados
â”œâ”€â”€ requirements.txt       # DependÃªncias Python
â”œâ”€â”€ README.md             # Este arquivo
â”œâ”€â”€ index/                # Ãndice FAISS (gerado automaticamente)
â”œâ”€â”€ embeddings/           # Metadados dos embeddings
â”œâ”€â”€ resultados/           # Resultados das comparaÃ§Ãµes (JSON)
â””â”€â”€ analises/             # AnÃ¡lises exportadas (CSV)
```

## ğŸš€ Como Usar

### 1. InstalaÃ§Ã£o das DependÃªncias

```bash
# Instalar dependÃªncias Python
pip install -r requirements.txt

# Ou instalar manualmente
pip install faiss-cpu sentence-transformers requests numpy pandas
```

### 2. ConfiguraÃ§Ã£o dos Modelos

```bash
# Executar script de configuraÃ§Ã£o automÃ¡tica
python setup_models.py
```

Este script irÃ¡:
- Verificar se o Ollama estÃ¡ instalado
- Verificar se o servidor Ollama estÃ¡ rodando
- Baixar automaticamente os modelos necessÃ¡rios:
  - `llama2:3b` (LLM de 3B parÃ¢metros)
  - `tinyllama` (LLM de 1B parÃ¢metros)

### 3. PreparaÃ§Ã£o dos Dados

Certifique-se de que as obras estÃ£o organizadas:
```
/home/umbelito/IPCC/obras/raw/txt/
â”œâ”€â”€ romance/
â”‚   â”œâ”€â”€ dom_casmurro.txt
â”‚   â”œâ”€â”€ memorias_postumas.txt
â”‚   â””â”€â”€ ...
â””â”€â”€ cronica/
    â”œâ”€â”€ cronica1.txt
    â”œâ”€â”€ cronica2.txt
    â””â”€â”€ ...
```

### 4. ExecuÃ§Ã£o do Teste Comparativo

```bash
python machado_rag.py
```

### 5. AnÃ¡lise dos Resultados

```bash
python analisar_resultados.py
```

## ğŸ“Š Funcionalidades

### Script Principal (`machado_rag.py`)
- ComparaÃ§Ã£o lado a lado das duas abordagens
- MediÃ§Ã£o de tempo de resposta
- Salvamento automÃ¡tico de resultados
- Interface interativa para perguntas

### Script de Setup (`setup_models.py`)
- VerificaÃ§Ã£o automÃ¡tica do ambiente
- Download automÃ¡tico dos modelos
- ValidaÃ§Ã£o da configuraÃ§Ã£o

### Script de AnÃ¡lise (`analisar_resultados.py`)
- AnÃ¡lise estatÃ­stica dos tempos
- CategorizaÃ§Ã£o das perguntas
- ExportaÃ§Ã£o para CSV
- RelatÃ³rios de performance

## ğŸ”§ ConfiguraÃ§Ãµes

No arquivo `machado_rag.py`, vocÃª pode ajustar:

```python
# Modelos
LLM_3B_MODEL = "llama2:3b"    # Modelo de 3B parÃ¢metros
LLM_1B_MODEL = "tinyllama"    # Modelo de 1B parÃ¢metros

# ConfiguraÃ§Ãµes RAG
TOP_K = 5                     # NÃºmero de trechos relevantes
EMBEDDING_MODEL = "all-MiniLM-L6-v2"  # Modelo de embeddings
```

## ğŸ“ˆ Resultados

### Formato de SaÃ­da
Os resultados sÃ£o salvos em JSON com:
- Timestamp da comparaÃ§Ã£o
- Pergunta realizada
- Respostas de ambas as abordagens
- Tempos de resposta
- Metadados dos modelos

### AnÃ¡lises DisponÃ­veis
- **Tempo de resposta**: MÃ©dia, mÃ­nimo, mÃ¡ximo
- **ComparaÃ§Ã£o de performance**: Qual abordagem Ã© mais rÃ¡pida
- **CategorizaÃ§Ã£o de perguntas**: Por tipo de conteÃºdo
- **ExportaÃ§Ã£o para CSV**: Para anÃ¡lise detalhada

## ğŸ¯ Casos de Uso

Este teste Ã© Ãºtil para:
- Avaliar trade-offs entre tamanho do modelo e qualidade
- Comparar eficiÃªncia de RAG vs geraÃ§Ã£o pura
- Analisar impacto do contexto especÃ­fico na qualidade das respostas
- Benchmark de performance em tarefas especÃ­ficas de literatura
- Pesquisa em sistemas de recuperaÃ§Ã£o de informaÃ§Ã£o

## ğŸ” Exemplo de SaÃ­da

```
ğŸ”¬ TESTE COMPARATIVO: LLM 3B vs LLM 1B + RAG
============================================================
ğŸ“– LLM de 3B parÃ¢metros: llama2:3b (SEM RAG)
ğŸ” LLM de 1B parÃ¢metros: tinyllama (COM RAG)
============================================================

ğŸ¤” Pergunta: Quem Ã© o protagonista de Dom Casmurro?
================================================================================

ğŸ“š LLM de 3B parÃ¢metros (SEM RAG):
--------------------------------------------------
â±ï¸  Tempo de resposta: 2.34s
ğŸ’¬ Resposta: [Resposta baseada no conhecimento prÃ©-treinado]

ğŸ” LLM de 1B parÃ¢metros (COM RAG):
--------------------------------------------------
â±ï¸  Tempo de resposta: 1.87s
ğŸ“– Trechos relevantes encontrados: 5
ğŸ’¬ Resposta: [Resposta baseada nos trechos recuperados]

ğŸ“Š COMPARAÃ‡ÃƒO:
--------------------------------------------------
ğŸ• Tempo 3B (sem RAG): 2.34s
ğŸ• Tempo 1B (com RAG): 1.87s
âš¡ DiferenÃ§a: 0.47s
ğŸ† LLM de 1B com RAG foi mais rÃ¡pida!
```

## ğŸ› ï¸ SoluÃ§Ã£o de Problemas

### Ollama nÃ£o encontrado
```bash
# Instalar Ollama
curl -fsSL https://ollama.ai/install.sh | sh
```

### Servidor Ollama nÃ£o estÃ¡ rodando
```bash
# Iniciar servidor
ollama serve
```

### Modelos nÃ£o encontrados
```bash
# Executar setup automÃ¡tico
python setup_models.py
```

### Erro de dependÃªncias
```bash
# Reinstalar dependÃªncias
pip install -r requirements.txt --upgrade
```

## ğŸ“ Notas TÃ©cnicas

- O Ã­ndice FAISS Ã© criado automaticamente na primeira execuÃ§Ã£o
- Os embeddings sÃ£o gerados usando o modelo `all-MiniLM-L6-v2`
- Os resultados sÃ£o salvos automaticamente na pasta `resultados/`
- As anÃ¡lises sÃ£o exportadas para CSV na pasta `analises/`