#!/usr/bin/env python3
"""
Script para configurar os modelos necess√°rios para o teste comparativo
"""

import subprocess
import sys
import time

def run_command(command, description):
    """Executa um comando e mostra o progresso"""
    print(f"\nüîÑ {description}...")
    print(f"Comando: {command}")
    
    try:
        result = subprocess.run(command, shell=True, capture_output=True, text=True)
        if result.returncode == 0:
            print(f"‚úÖ {description} conclu√≠do com sucesso!")
            return True
        else:
            print(f"‚ùå Erro ao {description.lower()}:")
            print(f"Erro: {result.stderr}")
            return False
    except Exception as e:
        print(f"‚ùå Exce√ß√£o ao {description.lower()}: {str(e)}")
        return False

def check_ollama():
    """Verifica se o Ollama est√° instalado e rodando"""
    print("üîç Verificando se o Ollama est√° instalado...")
    
    try:
        result = subprocess.run("ollama --version", shell=True, capture_output=True, text=True)
        if result.returncode == 0:
            print(f"‚úÖ Ollama encontrado: {result.stdout.strip()}")
            return True
        else:
            print("‚ùå Ollama n√£o encontrado no PATH")
            return False
    except FileNotFoundError:
        print("‚ùå Ollama n√£o est√° instalado")
        return False

def check_ollama_server():
    """Verifica se o servidor Ollama est√° rodando"""
    print("üîç Verificando se o servidor Ollama est√° rodando...")
    
    try:
        import requests
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code == 200:
            print("‚úÖ Servidor Ollama est√° rodando!")
            return True
        else:
            print("‚ùå Servidor Ollama n√£o est√° respondendo corretamente")
            return False
    except Exception as e:
        print(f"‚ùå Erro ao conectar com servidor Ollama: {str(e)}")
        return False

def install_models():
    """Instala os modelos necess√°rios"""
    models = [
        ("llama2:3b", "LLM de 3B par√¢metros (sem RAG)"),
        ("tinyllama", "LLM de 1B par√¢metros (com RAG)")
    ]
    
    print("\nüì¶ Instalando modelos necess√°rios...")
    print("=" * 60)
    
    for model, description in models:
        print(f"\nüìö {description}")
        print(f"Modelo: {model}")
        
        success = run_command(f"ollama pull {model}", f"Baixando {model}")
        
        if success:
            print(f"‚úÖ {model} instalado com sucesso!")
        else:
            print(f"‚ùå Falha ao instalar {model}")
            return False
        
        # Pequena pausa entre downloads
        time.sleep(1)
    
    return True

def list_installed_models():
    """Lista os modelos instalados"""
    print("\nüìã Modelos instalados:")
    print("-" * 30)
    
    try:
        result = subprocess.run("ollama list", shell=True, capture_output=True, text=True)
        if result.returncode == 0:
            print(result.stdout)
        else:
            print("‚ùå Erro ao listar modelos")
    except Exception as e:
        print(f"‚ùå Erro: {str(e)}")

def main():
    """Fun√ß√£o principal"""
    print("üîß CONFIGURA√á√ÉO DOS MODELOS PARA TESTE COMPARATIVO")
    print("=" * 60)
    
    # Verificar Ollama
    if not check_ollama():
        print("\n‚ùå Ollama n√£o est√° instalado.")
        print("üì• Instale o Ollama em: https://ollama.ai/")
        return False
    
    # Verificar servidor
    if not check_ollama_server():
        print("\n‚ùå Servidor Ollama n√£o est√° rodando.")
        print("üöÄ Inicie o servidor com: ollama serve")
        return False
    
    # Instalar modelos
    if not install_models():
        print("\n‚ùå Falha na instala√ß√£o dos modelos.")
        return False
    
    # Listar modelos
    list_installed_models()
    
    print("\nüéâ Configura√ß√£o conclu√≠da!")
    print("‚úÖ Voc√™ pode agora executar: python machado_rag.py")
    
    return True

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1) 